{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef834a65-1bbd-4f5d-982a-7e5666f2da3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b617dff6-9d44-4b56-bedb-10e192e34c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_image(csv_path, image_path, image_size=(512, 512)):\n",
    "    \"\"\"\n",
    "    Convert a CSV file containing polyline data to a PNG image.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, header=None)\n",
    "    \n",
    "    # Initialize image\n",
    "    fig, ax = plt.subplots(figsize=(image_size[0] / 100, image_size[1] / 100), dpi=100)\n",
    "    ax.set_xlim(0, image_size[0])\n",
    "    ax.set_ylim(0, image_size[1])\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "\n",
    "    polylines = {}\n",
    "    for index, row in df.iterrows():\n",
    "        polyline_id = int(row[0])\n",
    "        num_points = int(row[1])\n",
    "        x_coords = list(map(float, row[2:2+num_points]))\n",
    "        y_coords = list(map(float, row[2+num_points:2+2*num_points]))\n",
    "\n",
    "        if len(x_coords) > 1 and len(y_coords) > 1:\n",
    "            polylines[polyline_id] = (x_coords, y_coords)\n",
    "    \n",
    "    for (x_coords, y_coords) in polylines.values():\n",
    "        if len(x_coords) > 1 and len(y_coords) > 1:\n",
    "            polygon = Polygon(list(zip(x_coords, y_coords)), closed=False, edgecolor='black')\n",
    "            ax.add_patch(polygon)\n",
    "\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to match image coordinates\n",
    "    plt.savefig(image_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4702e6f0-92eb-4c9f-a7fd-ea9683345053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder, size=(512, 512)):\n",
    "    \"\"\"\n",
    "    Load images from a folder and resize them.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith('.png'):\n",
    "            img_path = os.path.join(folder, filename)\n",
    "            image = Image.open(img_path).resize(size).convert('RGB')\n",
    "            images.append(np.array(image))\n",
    "    return np.array(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4706c470-3258-4c81-9b1d-9fcee1ad46ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folders_and_convert_csv(input_csv_folder, output_csv_folder, input_image_folder, output_image_folder):\n",
    "    \"\"\"\n",
    "    Create folders for images and convert CSVs to PNG images.\n",
    "    \"\"\"\n",
    "    os.makedirs(input_image_folder, exist_ok=True)\n",
    "    os.makedirs(output_image_folder, exist_ok=True)\n",
    "    \n",
    "    # Convert input CSVs to input PNG images\n",
    "    for csv_file in os.listdir(input_csv_folder):\n",
    "        if csv_file.endswith('.csv'):\n",
    "            csv_path = os.path.join(input_csv_folder, csv_file)\n",
    "            image_path = os.path.join(input_image_folder, csv_file.replace('.csv', '.png'))\n",
    "            csv_to_image(csv_path, image_path)\n",
    "    \n",
    "    # Convert output CSVs to output PNG images\n",
    "    for csv_file in os.listdir(output_csv_folder):\n",
    "        if csv_file.endswith('.csv'):\n",
    "            csv_path = os.path.join(output_csv_folder, csv_file)\n",
    "            image_path = os.path.join(output_image_folder, csv_file.replace('.csv', '.png'))\n",
    "            csv_to_image(csv_path, image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e36e4d16-da73-4fd2-a1b0-add72ee88a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "input_csv_folder = 'input_csv'\n",
    "output_csv_folder = 'output_csv'\n",
    "input_image_folder = 'input_images'\n",
    "output_image_folder = 'output_images'\n",
    "predictions_folder = 'predictions'\n",
    "\n",
    "# Create folders and convert CSVs\n",
    "create_folders_and_convert_csv(input_csv_folder, output_csv_folder, input_image_folder, output_image_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3679276f-6476-4442-97d7-9c031a2dfa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "x_images = load_images_from_folder(input_image_folder)\n",
    "y_images = load_images_from_folder(output_image_folder)\n",
    "\n",
    "# Ensure same number of images and correct shapes\n",
    "assert x_images.shape[0] == y_images.shape[0], \"Number of input and output images must match\"\n",
    "assert x_images.shape[1:] == (512, 512, 3), \"Image dimensions must match the specified size\"\n",
    "\n",
    "# Normalize images\n",
    "x_images = x_images / 255.0\n",
    "y_images = y_images / 255.0\n",
    "\n",
    "# Split data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_images, y_images, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "948a128f-44ce-4ae0-872b-9094c86023f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "model = Sequential([\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(512, 512, 3)),\n",
    "    MaxPooling2D((2, 2), padding='same'),\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2, 2), padding='same'),\n",
    "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2, 2), padding='same'),\n",
    "    Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2, 2), padding='same'),\n",
    "    \n",
    "    Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "    UpSampling2D((2, 2)),\n",
    "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    UpSampling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    UpSampling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    UpSampling2D((2, 2)),\n",
    "    Conv2D(3, (3, 3), activation='sigmoid', padding='same')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='mse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "436d67ca-5d53-4a04-9246-aec5ec3a18e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3/3 [==============================] - 19s 5s/step - loss: 0.1072 - val_loss: 3.6271e-07\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 15s 5s/step - loss: 1.2804e-07 - val_loss: 4.0771e-17\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 1.3590e-17 - val_loss: 0.0000e+00\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 15s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 15s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 16s 6s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 15s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 15s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 15s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 15s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 15s 5s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x25c2c34cf50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Training\n",
    "model.fit(x_train, y_train, epochs=50, batch_size=1, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23515f4b-1918-421a-80c7-fc7697a6a249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 980ms/step - loss: 0.0000e+00\n",
      "Test loss: 0.0\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "Predictions shape: (1, 512, 512, 3)\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "loss = model.evaluate(x_test, y_test)\n",
    "print(f\"Test loss: {loss}\")\n",
    "\n",
    "# Model Prediction\n",
    "predictions = model.predict(x_test)\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "\n",
    "# Create predictions folder if it does not exist\n",
    "os.makedirs(predictions_folder, exist_ok=True)\n",
    "\n",
    "# Save predictions as images\n",
    "for i, prediction in enumerate(predictions):\n",
    "    pred_img = (prediction * 255).astype(np.uint8)\n",
    "    Image.fromarray(pred_img).save(f'{predictions_folder}/prediction_{i}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40afb26b-58e8-4961-9356-c2ab1f40feff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
